configfile: "config/config.yaml"


rule all:
    input:
        # 'results/SRA.feather',
        # 'results/metadata.csv',
        'logs/processed_stats_per_platform.ipynb'



rule query_ncbi:
    """
    Download or use already present NCBI SRA database dump to 
    query for SRA samples
    """
    output:
        feather_file = "results/SRA.feather"
    params:
        database = config['database'],
        taxon = config['taxon_id']
    conda: "envs/Renv.yml"
    log: "logs/query_ncbi.log"
    threads: 1
    shell:
        r'''
        (workflow/scripts/retrieve_NCBI_metadata.R \
            --database {params.database} \
            --taxon_id {params.taxon} \
            --output {output.feather_file}) >{log} 2>&1
        '''


rule wrangle_metadata:
    """
    Wrangles metadata from NCBI so that all runs have a isolation source
    geolocation and collection date.
    """
    input:
        feather_file = rules.query_ncbi.output.feather_file
    output:
        metadata = 'results/metadata.csv',
        clean_tsv = 'results/clean_tsv.tsv'
    conda: "envs/NCBI_metadata_notebook.yml"
    threads: 1
    log: notebook="logs/processed_notebook.ipynb"
    notebook: "notebooks/wrangle_NCBI_metadata.py.ipynb"
    # log: "logs/wrangle_ncbi_metadata.log"
    # shell:
    #     '''
    #     (papermill workflow/notebooks/wrangle_NCBI_metadata.py.ipynb \
    #         -p file_path {input.feather_file} \
    #         -p illumina_out {output.illumina} \
    #         -p nanopore_out {output.nanopore} \
    #         -p pacbio_out {output.pacbio}) >{log} 2>&1
    #     '''

rule platformm_stats:
    """
    Runs a Jupyter notebook that creates some statistics about the species in the metadata per sequencing platform/
    """
    input:
        metadata = rules.wrangle_metadata.output.metadata
    output:
        # pdf = "results/stats.pdf",
        notebook = "logs/processed_stats_per_platform.ipynb"
    conda: "envs/stats_notebook.yml"
    threads: 1
    log: notebook="logs/processed_stats_per_platform.ipynb"
    notebook: "notebooks/stats_per_platform.py.ipynb"


rule download_files:
    """
    Downloads FASTA / FASTQ files for ID's collected in wrangle_metadata with SRAtools
    """
    input:
        metadata = rules.wrangle_metadata.output.clean_tsv
    params:
        download_dir = "results/SRA_downloads",
        platforms = config['download_platforms']
    output: "results/SRA_downloads/done.txt"
    conda: "envs/SRAtools.yml"
    threads: 6
    log: "logs/download_files.log"
    shell:
        '''
        (bash workflow/scripts/download_sra_files.sh \
            -f {input.metadata} \
            -o {params.download_dir} \
            {params.platforms}
        ) >{log} 2>&1'''

# rule download_files:
#     input:
#         metadata = rules.wrangle_metadata.output.clean_tsv
#     output:
#         directory("results/downloads"),
#         expand("results/downloads/{sample}.fastq.gz", sample=expand("{sample}", sample=open("accessions.txt").read().strip().split()))
#     params:
#         sra_accession=lambda wildcards: wildcards.sample
#     shell:
#         "cat {input} | xargs -I {{}} fastq-dump --outdir downloads/ --split-files {{}}"
