{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bc4d739aa7cfad",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "To do downstream analysis later in the project we need:\n",
    "1. Geolocation of where the sample was taken\n",
    "2. Source that the sample was isolated from\n",
    "3. Date on which the sample was collected\n",
    "4. Filter out lab strains\n",
    "\n",
    "None of these things have a column in any of the tables from the NCBI database. See this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6380228/) that describes how metadata in NCBI sucks.\n",
    " \n",
    "However, there is a column called 'sample_attribute' in the SRA and Sample table where a submitter can add additional information about a sample. As 'sample_attribute' does not require a specific format or specific information. The information found there varies greatly between samples. Some organizations (rivm) that submit data to the NCBI have a consisted format for this column which then also varies per organization, others do not. This makes it very challenging to extrapolate the information mentioned above for all samples. In this notebook we attempt to extract this information."
   ]
  },
  {
   "cell_type": "code",
   "id": "73d9d16141f0b738",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pyarrow.feather as feather\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c20e0d8fb7e6c47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Functions written for this notebook are stored in wrangling_funcs.py. Please look there for documentation and tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": "import wrangling_funcs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5206359fbfd76a73",
   "metadata": {},
   "source": [
    "# Reading in the data\n",
    "---\n",
    "\n",
    "R has a nice package called SRAdb that you can use to query the NCBI database. However, I prefer working in Python. So we are querying the data in R using SRAdb and then exporting it in feather format for use here. There might be a way to directly get a dump of the SRA database and query it without using SRAdb. I will look into this.\n",
    "\n",
    "The default index of a dataframe is not useful to us. Instead, we use the run_accession, these should be unique. This way we can keep track when we split the metadata into a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "id": "91c8e35744180e75",
   "metadata": {},
   "source": [
    "# file_path = '../../results/SRA.feather'\n",
    "# data = feather.read_feather(file_path)\n",
    "data = feather.read_feather(snakemake.input[0])\n",
    "\n",
    "metadata_df = pd.DataFrame(data)\n",
    "metadata_df = metadata_df.convert_dtypes()\n",
    "metadata_df.set_index('run_accession', inplace=True)\n",
    "\n",
    "print(f'---Number of rows: {metadata_df.shape[0]}, Number of columns: {metadata_df.shape[1]}---')\n",
    "metadata_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "na_synonyms = {r'^\\*$', r'^-$', r'^\\.$', r'^[Nn]one$', r'^[Nn]an$', r'^[Uu]nknown$', r'(?i)^not[ _-]collected$', r'(?i)^not[ _-]provided', r'^\\?$', r'^ $', r'(?i)^not[ _-]applicable$', r'^[Nn]a$', r'^[Nn]o$', r'^[Oo]ther$', r'^[Mm]is{1,3}ing$', r'^[Uu]nspecified$', r'^[Nn]ot[ ]available$', r'^[Nn]ot[ :]available[:] not collected$', '^[Nn]ot[ :]available[:] to be reported later$'}\n",
    "\n",
    "metadata_df = metadata_df.replace(to_replace=na_synonyms, value=pd.NA, regex=True)\n",
    "\n",
    "print(f'---Number of rows: {metadata_df.shape[0]}, Number of columns: {metadata_df.shape[1]}---')\n",
    "metadata_df.head()"
   ],
   "id": "e20423edb39f0a0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "158c7f6169f29941",
   "metadata": {},
   "source": [
    "# Finding metadata in the sample_attribute\n",
    "---\n",
    "\n",
    "All the metadata we are interested in is contained in the 'sample_attribute' column. From what we could see most of the information in this column is split by '||' characters. The information between these characters is then often split using ':'. We will use this to make key value pairs which we will then turn into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "id": "7a828f293d94d74e",
   "metadata": {},
   "source": [
    "sample_attribute = metadata_df['sample_attribute']\n",
    "faulty_lines = []\n",
    "correct_lines = []\n",
    "\n",
    "pattern = re.compile(r\"^[mM]is{1,3}ing$|^[nN]ot.*|^[oO]ther$|^[uU]nspecified$|^\\.$|^\\*$|\\?|^[Nn]a[nN]$|^[Nn]a$|^ $|^[Uu]nknown$|^[Nn]o$\")\n",
    "\n",
    "for line, identity in zip(sample_attribute, sample_attribute.index):\n",
    "    line = line.split(\"||\")\n",
    "    line_items = defaultdict(list)\n",
    "    for subitem in line:\n",
    "        try:\n",
    "            key, value = subitem.split(': ', 1)\n",
    "            strip_key = wrangling_funcs.clean_string(key)\n",
    "            strip_value = value.strip()\n",
    "            if pattern.match(strip_value):\n",
    "                strip_value = pd.NA\n",
    "            line_items[strip_key] = strip_value\n",
    "            line_items['run_accession'] = identity\n",
    "        except ValueError:\n",
    "            faulty_lines.append((identity, line))\n",
    "    correct_lines.append(line_items)\n",
    "\n",
    "smpl_att_df = pd.DataFrame(correct_lines)\n",
    "smpl_att_df = smpl_att_df.convert_dtypes()\n",
    "smpl_att_df.set_index('run_accession', inplace=True)\n",
    "\n",
    "print(f'---Number of rows: {smpl_att_df.shape[0]}, Number of columns: {smpl_att_df.shape[1]}---')\n",
    "smpl_att_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"results/removed_samples.txt\", \"w\") as file:\n",
    "    for line in faulty_lines:\n",
    "        file.write(f\"{faulty_lines}\\n\")"
   ],
   "id": "dbfc08c69deb0e88",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df47fa10046a4a4c",
   "metadata": {},
   "source": [
    "### Searching for geographic data\n",
    "\n",
    "There is no consistent column that contains the geolocation. To (hopefully) obtain the geolocation we use regex to find keywords in the column names of the dataframe. The matched columns are then combined in a single column while handling NaN values.i\n",
    "\n",
    "![NCBI geo location description](images/geo_location.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "865003592205b67c",
   "metadata": {},
   "source": [
    "geo_col_matches = wrangling_funcs.find_columns(['geo', 'geographic', 'country', 'continent'], smpl_att_df, ['longitude', 'latitude', 'depth'])\n",
    "print(f'The following columns matched the keywords: {geo_col_matches}')\n",
    "\n",
    "smpl_att_df = wrangling_funcs.combine_columns(smpl_att_df, list(geo_col_matches), 'inferred_location')\n",
    "smpl_att_df.drop(geo_col_matches, inplace=True, axis=1)\n",
    "\n",
    "smpl_att_df['inferred_continent'], smpl_att_df['inferred_country'], smpl_att_df['inferred_city'] = zip(*smpl_att_df['inferred_location'].map(wrangling_funcs.clean_geo))\n",
    "smpl_att_df.drop('inferred_location', axis=1, inplace=True)\n",
    "\n",
    "smpl_att_df = smpl_att_df.convert_dtypes()\n",
    "\n",
    "print(f'---Number of rows: {smpl_att_df.shape[0]}, Number of columns: {smpl_att_df.shape[1]}---')\n",
    "smpl_att_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0d90f0a9eee1b76",
   "metadata": {},
   "source": [
    "### Searching for the sample collection data\n",
    "\n",
    "There is no consistent column that contains the date. To (hopefully) obtain the date we use regex to find keywords in the column names of the dataframe. The matched columns are then combined in a single column while handling NaN values.\n",
    "\n",
    "![ncbi collection date description](images/collection_date.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "30aec547a3f29fc5",
   "metadata": {},
   "source": [
    "date_col_matches = wrangling_funcs.find_columns(['date', 'year', 'time'], smpl_att_df, ['update'])\n",
    "print(f'The following columns matched the keywords: {date_col_matches}')\n",
    "smpl_att_df = wrangling_funcs.combine_columns(smpl_att_df, list(date_col_matches), 'inferred_collection_year')\n",
    "smpl_att_df.drop(date_col_matches, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "date = smpl_att_df['inferred_collection_year'].str.extract(r'^(\\d{4})', expand=False) # Extract the year\n",
    "smpl_att_df['inferred_collection_year'] = pd.to_numeric(date) # cast year to int\n",
    "\n",
    "smpl_att_df = smpl_att_df.convert_dtypes()\n",
    "print(f'---Number of rows: {smpl_att_df.shape[0]}, Number of columns: {smpl_att_df.shape[1]}---')\n",
    "smpl_att_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "379520e6ff30f8d4",
   "metadata": {},
   "source": [
    "### Searching for sample isolation source\n",
    "There is no consistent column that contains the isolation source. To (hopefully) obtain the isolation source we use regex to find keywords in the column names of the dataframe. The matched columns are then combined in a single column while handling NaN values.\n",
    "\n",
    "![NCBI isolation source description](images/isolation_source.png)\n",
    "![NCBI env package description](images/env_package.png)\n",
    "![NCBI isolation name description](images/isolate_name.png)\n",
    "![NCBI relative location description](images/rel_location.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "79b603a0a61f7aad",
   "metadata": {},
   "source": [
    "isolate_matches = wrangling_funcs.find_columns(['sample', 'source', 'environment', 'env', 'site'], smpl_att_df, ['name', 'provider', 'comment'])\n",
    "# isolate_matches = wrangling_funcs.find_columns(['source', 'isolate'], smpl_att_df, ['name', 'provider', 'comment', 'time', 'date', 'collected'])\n",
    "print(isolate_matches)\n",
    "\n",
    "smpl_att_df = wrangling_funcs.combine_columns(smpl_att_df, list(isolate_matches), \"inferred_source\")\n",
    "smpl_att_df.drop(isolate_matches, inplace=True, axis=1)\n",
    "\n",
    "smpl_att_df['inferred_source'] = smpl_att_df['inferred_source'].apply(wrangling_funcs.clean_source)\n",
    "\n",
    "smpl_att_df = smpl_att_df.convert_dtypes()\n",
    "print(f'---Number of rows: {smpl_att_df.shape[0]}, Number of columns: {smpl_att_df.shape[1]}---')\n",
    "smpl_att_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40fd0a97f2738fc",
   "metadata": {},
   "source": [
    "### Remove non relevant columns\n",
    "We have a ton of columns and very few of them are actually usefull to us. Let's remove all not relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "97b0d35d9e2d79a6",
   "metadata": {},
   "source": [
    "# latitude and longitude are optional\n",
    "if  'geographic_location_latitude' in smpl_att_df.columns and 'geographic_location_longitude' in smpl_att_df.columns:\n",
    "    smpl_att_df = smpl_att_df[['strain', 'inferred_collection_year', 'inferred_source', 'inferred_continent', 'inferred_country', 'inferred_city', 'geographic_location_latitude', 'geographic_location_longitude']]\n",
    "elif 'strain' in smpl_att_df.columns:\n",
    "    # Not guaranteed to be here\n",
    "    smpl_att_df = smpl_att_df[['strain', 'inferred_collection_year', 'inferred_source', 'inferred_continent', 'inferred_country', 'inferred_city']]\n",
    "else:\n",
    "    smpl_att_df = smpl_att_df[['inferred_collection_year', 'inferred_source', 'inferred_continent', 'inferred_country', 'inferred_city']]\n",
    "\n",
    "\n",
    "print(f'---Number of rows: {smpl_att_df.shape[0]}, Number of columns: {smpl_att_df.shape[1]}---')\n",
    "smpl_att_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81a13abd9b64c06b",
   "metadata": {},
   "source": [
    "# Combine sample_attribute metadata with rest of the data\n",
    "---\n",
    "Now that we have extracted the metadata that we wanted we can combine it back to the original dataframe. We only want to keep rows that have values for the collection_year/source/country because we require this downstream."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9cbab3a752d8017",
   "metadata": {},
   "source": [
    "combined_df = metadata_df.join(smpl_att_df)\n",
    "cols = ['inferred_collection_year', 'inferred_source', 'inferred_country']\n",
    "combined_df = combined_df.dropna(subset=cols)\n",
    "\n",
    "combined_df = combined_df.convert_dtypes()\n",
    "print(f'---Number of rows: {combined_df.shape[0]}, Number of columns: {combined_df.shape[1]}---')\n",
    "combined_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b81ad3a186933b9",
   "metadata": {},
   "source": [
    "### Throw away empty columns\n",
    "We want to filter out columns that only have NaN values so there is less cluter"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe3650674b5c6f7d",
   "metadata": {},
   "source": [
    "combined_df = combined_df.dropna(axis=1, how='all')\n",
    "\n",
    "print(f'---Number of rows: {combined_df.shape[0]}, Number of columns: {combined_df.shape[1]}---')\n",
    "combined_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a clean TSV using a selection of columns for parsing with bash when downloading through SRAtools"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f57f8c66896408a"
  },
  {
   "cell_type": "code",
   "source": [
    "clean_tsv = combined_df[['platform', 'scientific_name']].copy()\n",
    "clean_tsv['scientific_name'] = clean_tsv['scientific_name'].str.replace(' ', '_')\n",
    "clean_tsv.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3c175cfa2d2856",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write out files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397d6f39e9180158"
  },
  {
   "cell_type": "code",
   "source": [
    "clean_tsv.to_csv(snakemake.output[1], sep='\\t')\n",
    "combined_df.to_csv(snakemake.output[0], na_rep='NaN')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f390e5a77f5c29",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
